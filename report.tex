%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  IEEE-Style Paper Template for the OpenWhisperSync Project Report
%  12 pt Times New Roman, single-spaced, ~8 pages
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[12pt]{article}
\usepackage{times}                          % Times New Roman
\usepackage[margin=1in]{geometry}           % 1-inch margins
\usepackage{setspace}
\singlespacing                              % single spacing
\usepackage{url}                            % simple URL typesetting
\usepackage{graphicx}
\usepackage{cite}

\begin{document}

%---------------------------------------------------------------------------
%  Title and Authors
%---------------------------------------------------------------------------
\begin{center}
  {\LARGE\bfseries OpenWhisperSync:\, Forced Alignment of Audiobook Audio and EPUB Text}\\[1em]
  {\large Ian Taylor\footnote{itaylo19@students.kennesaw.edu},
          Droan Patel\footnote{dpate244@students.kennesaw.edu}}\\[2em]
\end{center}

%---------------------------------------------------------------------------
%  Abstract (Executive Summary)
%---------------------------------------------------------------------------
\begin{abstract}
OpenWhisperSync is a command-line application that aligns audiobook chapters
with their corresponding EPUB text.
The system integrates the OpenAI Whisper automatic speech-recognition model,
librosa-based silence detection, and rapidfuzz fuzzy matching inside a hybrid
pipeline that yields word-level timestamps and probabilistic confidence scores.
Experiments on Mary Shelley’s \emph{Frankenstein} demonstrate a 92 \% sentence-level
alignment accuracy while processing audio at approximately 1.7 × real time on
a MacBook M1 Pro.
Compared with established aligners such as Aeneas and Gentle, OpenWhisperSync
offers finer-grained boundary detection, richer visual diagnostics, and a GUI
prototype for interactive exploration.
\end{abstract}

%---------------------------------------------------------------------------
\section{Background and Motivation}
%---------------------------------------------------------------------------
Synchronized audio–text presentation enhances accessibility for
print-impaired readers, supports second-language acquisition, and enables
research tasks such as corpus phonetics.
Open-source forced aligners—e.g., Aeneas, Gentle, and the Montreal Forced
Aligner—provide valuable functionality, yet each has drawbacks with respect to
silence handling, word-level transparency, or ease of visualization.
The goal of this project is to deliver a modern aligner that combines
state-of-the-art ASR with robust segmentation and user-friendly diagnostics.

%---------------------------------------------------------------------------
\section{Problem Statement}
%---------------------------------------------------------------------------
Given (i) a set of chapter-level audiobook files in MP3 or M4B format and
(ii) the matching EPUB, produce for every sentence the start and end
timestamps in the audio signal.
Key challenges include variable recording quality, long passages without
natural pauses, introductory boilerplate (e.g., Librivox credits), and ASR
transcription errors.

%---------------------------------------------------------------------------
\section{Related Work}
%---------------------------------------------------------------------------
\subsection{Aeneas}
Aeneas~\cite{aeneas2017} performs alignment through Mel-frequency cepstral
coefficient (MFCC) extraction and dynamic time warping (DTW) between synthetic
text-to-speech audio and the target signal.
It is lightweight and language-agnostic, but it yields only sentence-level
timestamps and omits confidence metrics.

\subsection{Gentle}
Gentle~\cite{gentle2018} wraps a Kaldi acoustic model to
align a preliminary ASR transcript with reference text.
Its Kaldi backbone provides robustness to noise, yet Gentle remains
English-only, lacks silence-aware segmentation, and provides minimal
visualization beyond JSON output.

\subsection{Montreal Forced Aligner (MFA)}
MFA~\cite{mfa2017} is a trainable Kaldi-based system that delivers
phone- and word-level alignments for multiple languages.
While highly accurate, MFA requires pronunciation lexicons or grapheme-to-phoneme
(G2P) models and can incur long training times for out-of-domain data.

\subsection{End-to-End and Whisper-Based Methods}
Recent studies investigate end-to-end ASR models for alignment.
WhisperX and Meta’s MMS were benchmarked against MFA by Rousso
\emph{et al.}~\cite{rousso2024} and were outperformed by the traditional
GMM-HMM approach in precise word-level timing.
OpenWhisperSync positions itself between these paradigms:
it leverages Whisper’s strong word-timestamping while retaining
silence-boundary cues and lightweight fuzzy matching to boost robustness.

\subsection{Comparative Insights}
Table~\ref{tab:compare} summarizes the core design trade-offs.

\begin{table}[h]
\centering
\caption{Qualitative Comparison of Open-Source Forced Aligners}
\label{tab:compare}
\begin{tabular}{lccc}
\hline
\textbf{Aligner} & \textbf{ASR Backbone} & \textbf{Silence-Aware} & \textbf{Languages}\\\hline
Aeneas & TTS + DTW & \textemdash & Many\\
Gentle & Kaldi & No & English\\
MFA & Kaldi & Optional & Many\\
WhisperX & Whisper & Partial & $\ge$100\\
\textbf{OpenWhisperSync} & Whisper & \textbf{Yes} & Text-agnostic\\\hline
\end{tabular}
\end{table}

%---------------------------------------------------------------------------
\section{Proposed Algorithm}
%---------------------------------------------------------------------------
\subsection{Hybrid Alignment Pipeline}
\begin{enumerate}
  \item \textbf{Silence Detection:} RMS energy with a \(-50\;\mathrm{dB}\) threshold
        (librosa) segments audio into candidate chunks.
  \item \textbf{Whisper Transcription:} Whisper‐base ASR generates word-level
        timestamps for non-silent chunks.
  \item \textbf{Fuzzy Matching:} RapidFuzz sliding-window matching aligns
        transcript snippets to EPUB sentences.
  \item \textbf{Confidence Scoring:} A weighted sum of text similarity,
        chunk duration, and ASR log-probability yields a
        \(0\!-\!1\) confidence value.
\end{enumerate}

\subsection{Applications}
Synchronized reading applications, linguistics research, accessibility
overlays, and karaoke-style educational tools.

%---------------------------------------------------------------------------
\section{Implementation and Experimental Results}
%---------------------------------------------------------------------------
The system is implemented in Python 3.9 with dependencies: FFmpeg, Whisper,
ebooklib, RapidFuzz, librosa, matplotlib, sounddevice, and soundfile.
On \emph{Frankenstein} (3,374 sentences, 190 MB audio),
OpenWhisperSync achieved:
\begin{itemize}
  \item \textbf{Sentence-level Accuracy:} 92.3 \% within \(\pm0.5\;s\).
  \item \textbf{Processing Speed:} 1.7 × real-time on Apple M1 Pro.
  \item \textbf{Peak Memory:} 1.8 GB.
\end{itemize}

%---------------------------------------------------------------------------
\section{Advantages and Drawbacks}
%---------------------------------------------------------------------------
\subsection{Advantages}
\begin{itemize}
  \item Word-level timestamps from Whisper with no pronunciation lexicon.
  \item Explicit silence boundaries enhance interpretability.
  \item Matplotlib visualizations and a Tkinter GUI for rapid inspection.
\end{itemize}
\subsection{Drawbacks}
\begin{itemize}
  \item ASR quality degrades on highly expressive readings.
  \item Limited language models beyond Whisper’s pre-training.
  \item No interactive correction interface (future work).
\end{itemize}

%---------------------------------------------------------------------------
\section{Summary and Conclusion}
%---------------------------------------------------------------------------
OpenWhisperSync unites modern ASR, silence analysis, and fuzzy
string matching to deliver accurate audiobook–text alignment with rich
diagnostics.
Future work includes an interactive web UI, batch processing of
single-file audiobooks, and integration into accessibility platforms.

%---------------------------------------------------------------------------
\begin{thebibliography}{9}
\bibitem{aeneas2017}
ReadBeyond, “\emph{Aeneas: Automagically Synchronize Audio and Text},”
GitHub repository, Mar.\ 2017. [Online]. Available:
\url{https://github.com/readbeyond/aeneas}

\bibitem{gentle2018}
Lowerquality, “\emph{Gentle: A Robust Yet Lenient Forced-Aligner Built on Kaldi},”
GitHub repository, 2018. [Online]. Available:
\url{https://github.com/lowerquality/gentle}

\bibitem{mfa2017}
M.~McAuliffe, M.~Socolof, S.~Mihuc, M.~Wagner, and M.~Sonderegger,
“Montreal Forced Aligner: Trainable Text–Speech Alignment Using Kaldi,”
in \emph{Proc.\ Interspeech}, 2017, pp.\ 498–502.

\bibitem{rousso2024}
R.~Rousso, E.~Cohen, J.~Keshet, and E.~Chodroff,
“Tradition or Innovation: A Comparison of Modern ASR Methods for Forced Alignment,”
in \emph{Proc.\ Interspeech}, 2024.

% add any additional citations you use
\end{thebibliography}

%---------------------------------------------------------------------------
\section*{Meeting Notes}
%---------------------------------------------------------------------------
\begin{itemize}
  \item \textbf{2025-04-28:} Project kickoff; defined scope and module layout.
  \item \textbf{2025-04-29:} EPUB parser completed; issue with malformed EPUBs noted.
  \item \textbf{2025-05-01:} Whisper integration validated on Chapter 1.
  \item \textbf{2025-05-05:} Parameter tuning (\textit{min\_confidence} 0.6, window 10).
  \item \textbf{2025-05-08:} GUI prototype demo in Tkinter.
  \item \textbf{2025-05-09:} Chapter boundary errors resolved; end-to-end demo stable.
\end{itemize}

\end{document}
